{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9dc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "from datetime import timezone\n",
    "import pytz\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.seisbench_models import create_detector, create_classifier, convert_picks_to_detections\n",
    "from src.detect import (\n",
    "    smooth_moving_avg, detect_event_windows, multi_class_detection,\n",
    "    classify_waveform_windows, merge_picks_and_classifications,\n",
    "    filter_noise_events, get_event_type_counts\n",
    ")\n",
    "from src.interactive_plots import plot_detection_results, interactive_detection_viewer, plot_event_summary\n",
    "from src.utils import ensure_dir\n",
    "\n",
    "# For interactive plots in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48599e2",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for UW.DREAM mudslide detection\n",
    "config = {\n",
    "    # Station settings\n",
    "    'network': 'UW',\n",
    "    'station': 'DREAM',\n",
    "    'channel': 'HH*',  # High-bandwidth seismometer\n",
    "    'location': '*',\n",
    "    \n",
    "    # Time window - last 5 days\n",
    "    'days_back': 5,\n",
    "    \n",
    "    # Picker model settings\n",
    "    'picker_model': 'phasenet',\n",
    "    'picker_version': 'stead',\n",
    "    \n",
    "    # Classifier model settings\n",
    "    'classifier_model': 'quakexnet',\n",
    "    'classifier_version': 'base',\n",
    "    \n",
    "    # Device settings\n",
    "    'device': 'auto',\n",
    "    \n",
    "    # Picker detection parameters\n",
    "    'picker_threshold': 0.5,\n",
    "    'min_duration': 10,\n",
    "    'merge_distance': 50,\n",
    "    'apply_smoothing': True,\n",
    "    'smooth_window': 100,\n",
    "    \n",
    "    # Classification parameters\n",
    "    'window_duration': 100.0,\n",
    "    'stride': 50.0,\n",
    "    'batch_size': 12,\n",
    "    'include_sliding': True,\n",
    "    'include_event_centered': True,\n",
    "    \n",
    "    # Merging parameters\n",
    "    'time_tolerance': 10.0,\n",
    "    \n",
    "    # Output settings\n",
    "    'save_plots': True,\n",
    "    'save_results': True,\n",
    "    'local_timezone': 'America/Los_Angeles',\n",
    "}\n",
    "\n",
    "# Calculate time window\n",
    "endtime = UTCDateTime.now()\n",
    "starttime = endtime - config['days_back'] * 24 * 3600\n",
    "\n",
    "print(f\"UW.DREAM Mudslide Detection\")\n",
    "print(f\"Time window: {starttime} to {endtime}\")\n",
    "print(f\"Duration: {config['days_back']} days ({(endtime - starttime) / 3600:.1f} hours)\")\n",
    "\n",
    "# Create output directories\n",
    "ensure_dir('../plots')\n",
    "ensure_dir('../logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf59cf",
   "metadata": {},
   "source": [
    "## 2. Download Data from IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfcbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download waveform data\n",
    "print(f\"Downloading data for {config['network']}.{config['station']}...\")\n",
    "\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "try:\n",
    "    stream = client.get_waveforms(\n",
    "        network=config['network'],\n",
    "        station=config['station'],\n",
    "        channel=config['channel'],\n",
    "        location=config['location'],\n",
    "        starttime=starttime,\n",
    "        endtime=endtime\n",
    "    )\n",
    "    \n",
    "    # Merge traces and fill gaps\n",
    "    stream.merge(method=1, fill_value=0)\n",
    "    \n",
    "    print(f\"✓ Downloaded {len(stream)} traces\")\n",
    "    print(f\"\\nStream info:\")\n",
    "    print(stream)\n",
    "    \n",
    "    # Get station inventory for response removal\n",
    "    print(\"\\nFetching station inventory...\")\n",
    "    inventory = client.get_stations(\n",
    "        network=config['network'],\n",
    "        station=config['station'],\n",
    "        channel=config['channel'],\n",
    "        location=config['location'],\n",
    "        starttime=starttime,\n",
    "        endtime=endtime,\n",
    "        level=\"response\"\n",
    "    )\n",
    "    print(\"✓ Inventory retrieved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error downloading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf029e95",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instrumental response\n",
    "print(\"Removing instrumental response...\")\n",
    "stream_corrected = stream.copy()\n",
    "stream_corrected.remove_response(inventory=inventory, output=\"VEL\", water_level=60)\n",
    "\n",
    "print(f\"✓ Instrumental response removed\")\n",
    "print(f\"Output: Velocity (m/s)\")\n",
    "\n",
    "# Plot raw waveforms\n",
    "local_tz = pytz.timezone(config['local_timezone'])\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "for i, tr in enumerate(stream_corrected):\n",
    "    # Efficient timezone conversion\n",
    "    start_datetime = tr.stats.starttime.datetime.replace(tzinfo=timezone.utc)\n",
    "    start_local = start_datetime.astimezone(local_tz)\n",
    "    times_local = [start_local + pd.Timedelta(seconds=float(t)) for t in tr.times()]\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].plot(times_local, tr.data, 'k-', linewidth=0.3, alpha=0.8)\n",
    "    axes[i].set_ylabel(f'{tr.stats.channel}\\nVelocity (m/s)', fontsize=10)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].text(0.01, 0.95, f'{tr.stats.network}.{tr.stats.station}.{tr.stats.location}.{tr.stats.channel}',\n",
    "                transform=axes[i].transAxes, fontsize=9, va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Format x-axis\n",
    "axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M', tz=local_tz))\n",
    "axes[-1].xaxis.set_major_locator(mdates.HourLocator(interval=12))\n",
    "plt.setp(axes[-1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "axes[-1].set_xlabel(f'Time ({local_tz.zone})', fontsize=11)\n",
    "\n",
    "fig.suptitle(f'UW.DREAM - {config[\"days_back\"]} Day Waveform Overview', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "if config['save_plots']:\n",
    "    save_path = f\"../plots/DREAM_{starttime.date}_waveforms.png\"\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Plot saved to {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef0042",
   "metadata": {},
   "source": [
    "## 4. Load Detection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PhaseNet picker\n",
    "print(f\"Loading {config['picker_model']} picker...\")\n",
    "picker = create_detector(\n",
    "    model_name=config['picker_model'],\n",
    "    version=config['picker_version'],\n",
    "    device=config['device']\n",
    ")\n",
    "print(f\"✓ Picker loaded on {picker.device}\")\n",
    "\n",
    "# Load QuakeXNet classifier\n",
    "print(f\"\\nLoading {config['classifier_model']} classifier...\")\n",
    "classifier = create_classifier(\n",
    "    model_name=config['classifier_model'],\n",
    "    version=config['classifier_version'],\n",
    "    device=config['device']\n",
    ")\n",
    "print(f\"✓ Classifier loaded\")\n",
    "print(f\"\\n✓ Both models ready for detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87473aec",
   "metadata": {},
   "source": [
    "## 5. Phase Picking with PhaseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c06a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running PhaseNet phase picker...\")\n",
    "\n",
    "# Run picker inference\n",
    "annotated_stream = picker.model.annotate(stream)\n",
    "print(f\"✓ Picker inference complete\")\n",
    "\n",
    "# Extract probability traces\n",
    "pick_probabilities = picker._extract_predictions(annotated_stream)\n",
    "\n",
    "print(f\"\\nPhase probabilities extracted:\")\n",
    "for phase, probs in pick_probabilities.items():\n",
    "    print(f\"  {phase}: {len(probs)} samples, max prob: {np.max(probs):.3f}\")\n",
    "\n",
    "# Get deterministic picks\n",
    "picks_output = picker.model.classify(stream)\n",
    "print(f\"\\nDeterministic picks: {len(picks_output.picks)} picks found\")\n",
    "\n",
    "if len(picks_output.picks) > 0:\n",
    "    print(f\"\\nFirst 10 picks:\")\n",
    "    for i, pick in enumerate(picks_output.picks[:10]):\n",
    "        print(f\"  {i+1}. {pick.phase} at {pick.peak_time} (confidence: {pick.peak_value:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b16955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize picker probabilities\n",
    "annotated_stream.plot(size=(900, 600))\n",
    "plt.suptitle('PhaseNet Phase Probabilities', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90675a",
   "metadata": {},
   "source": [
    "## 6. Detect Pick Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detecting pick windows from probabilities...\")\n",
    "\n",
    "# Apply threshold-based detection\n",
    "pick_detections = multi_class_detection(\n",
    "    pick_probabilities,\n",
    "    threshold=config['picker_threshold'],\n",
    "    min_duration=config['min_duration'],\n",
    "    merge_distance=config['merge_distance'],\n",
    "    apply_smoothing=config['apply_smoothing'],\n",
    "    smooth_window=config['smooth_window']\n",
    ")\n",
    "\n",
    "print(f\"✓ Pick detection complete\")\n",
    "print(f\"\\nDetected picks by phase:\")\n",
    "total_picks = 0\n",
    "for phase_name, phase_picks in pick_detections.items():\n",
    "    print(f\"  {phase_name}: {len(phase_picks)} picks\")\n",
    "    total_picks += len(phase_picks)\n",
    "\n",
    "print(f\"\\nTotal picks: {total_picks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f27e9",
   "metadata": {},
   "source": [
    "## 7. Event Classification with QuakeXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5236a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running QuakeXNet event classification...\")\n",
    "print(\"This will classify windows as: earthquake (eq), explosion (px), noise (no), or surface event (su)\")\n",
    "\n",
    "# Run hybrid classification\n",
    "classification_results = classify_waveform_windows(\n",
    "    stream=stream,\n",
    "    classifier=classifier,\n",
    "    picker_detections=pick_detections,\n",
    "    window_duration=config['window_duration'],\n",
    "    stride=config['stride'],\n",
    "    batch_size=config['batch_size'],\n",
    "    include_sliding=config['include_sliding'],\n",
    "    include_event_centered=config['include_event_centered']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Classification complete\")\n",
    "print(f\"Total windows classified: {len(classification_results)}\")\n",
    "\n",
    "# Show classification distribution\n",
    "if classification_results:\n",
    "    class_labels = [r['class_label'] for r in classification_results]\n",
    "    print(\"\\nClassification distribution:\")\n",
    "    print(f\"  Earthquakes (eq): {class_labels.count('eq')}\")\n",
    "    print(f\"  Explosions (px): {class_labels.count('px')}\")\n",
    "    print(f\"  Noise (no): {class_labels.count('no')}\")\n",
    "    print(f\"  Surface events (su): {class_labels.count('su')}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_classifications = pd.DataFrame(classification_results)\n",
    "    print(f\"\\nClassification results preview:\")\n",
    "    print(df_classifications.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912fd88",
   "metadata": {},
   "source": [
    "## 8. Merge Picks and Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging picks and classifications...\")\n",
    "\n",
    "# Prepare picker results for merging\n",
    "sampling_rate = stream[0].stats.sampling_rate\n",
    "picker_results = []\n",
    "\n",
    "for phase_name, phase_picks in pick_detections.items():\n",
    "    for pick in phase_picks:\n",
    "        pick_record = pick.copy()\n",
    "        pick_record['phase'] = phase_name\n",
    "        pick_record['class_name'] = phase_name\n",
    "        picker_results.append(pick_record)\n",
    "\n",
    "# Merge results\n",
    "df_merged = merge_picks_and_classifications(\n",
    "    picker_results=picker_results,\n",
    "    classification_results=classification_results,\n",
    "    time_tolerance=config['time_tolerance'],\n",
    "    sampling_rate=sampling_rate\n",
    ")\n",
    "\n",
    "print(f\"✓ Merging complete\")\n",
    "print(f\"\\nMerged events summary:\")\n",
    "print(f\"  Total events: {len(df_merged)}\")\n",
    "print(f\"  Matched (pick + class): {len(df_merged[df_merged['match_type'] == 'matched'])}\")\n",
    "print(f\"  Pick only: {len(df_merged[df_merged['match_type'] == 'pick_only'])}\")\n",
    "print(f\"  Classification only: {len(df_merged[df_merged['match_type'] == 'class_only'])}\")\n",
    "\n",
    "# Save merged results\n",
    "if config['save_results']:\n",
    "    csv_path = f\"../logs/DREAM_{starttime.date}_merged_events.csv\"\n",
    "    df_merged.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n✓ Merged results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb1f7b",
   "metadata": {},
   "source": [
    "## 9. Identify Surface Events (Mudslides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for surface events\n",
    "surface_events = df_merged[df_merged['class_label'] == 'su'].copy()\n",
    "\n",
    "print(f\"SURFACE EVENT DETECTION RESULTS\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Total surface events detected: {len(surface_events)}\")\n",
    "\n",
    "if len(surface_events) > 0:\n",
    "    # Sort by probability\n",
    "    surface_events = surface_events.sort_values('class_prob', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop surface events by confidence:\")\n",
    "    print(surface_events[['pick_time', 'pick_phase', 'class_prob', 'match_type']].head(20))\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Mean confidence: {surface_events['class_prob'].mean():.3f}\")\n",
    "    print(f\"  Max confidence: {surface_events['class_prob'].max():.3f}\")\n",
    "    print(f\"  Events with confidence > 0.8: {len(surface_events[surface_events['class_prob'] > 0.8])}\")\n",
    "    print(f\"  Events with confidence > 0.9: {len(surface_events[surface_events['class_prob'] > 0.9])}\")\n",
    "    \n",
    "    # Save surface events\n",
    "    if config['save_results']:\n",
    "        surface_csv = f\"../logs/DREAM_{starttime.date}_surface_events.csv\"\n",
    "        surface_events.to_csv(surface_csv, index=False)\n",
    "        print(f\"\\n✓ Surface events saved to {surface_csv}\")\n",
    "else:\n",
    "    print(\"\\nNo surface events detected in this time window.\")\n",
    "\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752c0df",
   "metadata": {},
   "source": [
    "## 10. Visualization - Event Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5abd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timeline plot of all event types\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Define colors for each event type\n",
    "colors = {\n",
    "    'eq': 'red',\n",
    "    'px': 'orange',\n",
    "    'no': 'gray',\n",
    "    'su': 'green'\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'eq': 'Earthquake',\n",
    "    'px': 'Explosion',\n",
    "    'no': 'Noise',\n",
    "    'su': 'Surface Event (Mudslide)'\n",
    "}\n",
    "\n",
    "# Plot each event type\n",
    "for event_type in ['eq', 'px', 'no', 'su']:\n",
    "    events = df_merged[df_merged['class_label'] == event_type]\n",
    "    if len(events) > 0:\n",
    "        times = [starttime + t for t in events['pick_time'].values]\n",
    "        probs = events['class_prob'].values\n",
    "        \n",
    "        # Convert to local timezone for plotting\n",
    "        times_datetime = [t.datetime.replace(tzinfo=timezone.utc).astimezone(local_tz) for t in times]\n",
    "        \n",
    "        ax.scatter(times_datetime, probs, \n",
    "                  c=colors[event_type], \n",
    "                  label=f\"{labels[event_type]} (n={len(events)})\",\n",
    "                  alpha=0.6,\n",
    "                  s=50 if event_type == 'su' else 20,\n",
    "                  edgecolors='black' if event_type == 'su' else 'none',\n",
    "                  linewidths=1.5 if event_type == 'su' else 0)\n",
    "\n",
    "ax.set_xlabel(f'Time ({local_tz.zone})', fontsize=12)\n",
    "ax.set_ylabel('Classification Confidence', fontsize=12)\n",
    "ax.set_title(f'UW.DREAM Event Timeline - {config[\"days_back\"]} Days', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M', tz=local_tz))\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(interval=12))\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if config['save_plots']:\n",
    "    save_path = f\"../plots/DREAM_{starttime.date}_event_timeline.png\"\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Plot saved to {save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fddb9",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18984bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UW.DREAM MUDSLIDE DETECTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Station: UW.DREAM\")\n",
    "print(f\"Time window: {starttime} to {endtime}\")\n",
    "print(f\"Duration: {config['days_back']} days\")\n",
    "\n",
    "print(f\"\\n--- Detection Results ---\")\n",
    "print(f\"Total events detected: {len(df_merged)}\")\n",
    "print(f\"\\nBy event type:\")\n",
    "class_counts = df_merged[df_merged['class_label'].notna()]['class_label'].value_counts()\n",
    "for event_type, count in class_counts.items():\n",
    "    print(f\"  {labels.get(event_type, event_type)}: {count}\")\n",
    "\n",
    "print(f\"\\n--- Surface Events (Mudslides) ---\")\n",
    "if len(surface_events) > 0:\n",
    "    high_conf = surface_events[surface_events['class_prob'] > 0.8]\n",
    "    print(f\"Total surface events: {len(surface_events)}\")\n",
    "    print(f\"High confidence (>0.8): {len(high_conf)}\")\n",
    "    \n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\nMost likely mudslide event:\")\n",
    "        best = surface_events.iloc[0]\n",
    "        event_time = starttime + best['pick_time']\n",
    "        print(f\"  Time: {event_time}\")\n",
    "        print(f\"  Confidence: {best['class_prob']:.3f}\")\n",
    "        print(f\"  Phase: {best.get('pick_phase', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No surface events detected\")\n",
    "\n",
    "print(f\"\\n--- Phase Picks ---\")\n",
    "phase_counts = df_merged[df_merged['pick_phase'].notna()]['pick_phase'].value_counts()\n",
    "for phase, count in phase_counts.items():\n",
    "    print(f\"  {phase}: {count}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd10eb",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Interpreting Results\n",
    "- **Surface events (su)** are the primary indicator of mudslides/landslides\n",
    "- High confidence scores (>0.8) indicate strong mudslide signatures\n",
    "- Look for clusters of surface events in time for sustained slope failure\n",
    "- Compare with local weather/precipitation data for correlation\n",
    "\n",
    "### Next Steps\n",
    "1. Examine waveforms around high-confidence surface events\n",
    "2. Check for spectral characteristics typical of mass movements\n",
    "3. Correlate with precipitation data and known slope failures\n",
    "4. Compare with nearby stations for spatial extent\n",
    "5. Export results for further geotechnical analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaia-hazlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
